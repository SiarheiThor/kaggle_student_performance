{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 20:06:20.657467: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-08 20:06:20.774936: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 20:06:21.285856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/thor_01/miniconda3/envs/ds2023/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL = ['event_name', 'name','fqid', 'room_fqid', 'text_fqid']\n",
    "NUMERICAL = ['elapsed_time','level','page','room_coor_x', 'room_coor_y', \n",
    "        'screen_coor_x', 'screen_coor_y', 'hover_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the unique list of user sessions in the validation dataset. We assigned \n",
    "# `session_id` as the index of our feature engineered dataset. Hence fetching \n",
    "# the unique values in the index column will give us a list of users in the \n",
    "# validation set.\n",
    "VALID_USER_LIST = df1_valid.index.unique()\n",
    "\n",
    "# Create a dataframe for storing the predictions of each question for all users\n",
    "# in the validation set.\n",
    "# For this, the required size of the data frame is: \n",
    "# (no: of users in validation set  x no of questions).\n",
    "# We will initialize all the predicted values in the data frame to zero.\n",
    "# The dataframe's index column is the user `session_id`s. \n",
    "prediction_df = pd.DataFrame(data=np.zeros((len(VALID_USER_LIST),18)), index=VALID_USER_LIST)\n",
    "\n",
    "# Create an empty dictionary to store the models created for each question.\n",
    "models = {}\n",
    "\n",
    "# Create an empty dictionary to store the evaluation score for each question.\n",
    "evaluation_dict ={}\n",
    "f1_scores = {}\n",
    "auc_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through questions 1 to 18 to train models for each question, evaluate\n",
    "# the trained model and store the predicted values.\n",
    "for q_no in range(1,19):\n",
    "    # Select level group for the question based on the q_no.\n",
    "    if q_no<=3:\n",
    "        train_df = df1_train\n",
    "        valid_df = df1_valid\n",
    "    elif q_no<=13:\n",
    "        train_df = df2_train\n",
    "        valid_df = df2_valid\n",
    "    elif q_no<=22:\n",
    "        train_df = df3_train\n",
    "        valid_df = df3_valid\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Filter the rows in the datasets based on the selected level group. \n",
    "    \n",
    "    train_users = train_df.index.values\n",
    "    valid_users = valid_df.index.values\n",
    "\n",
    "    # Select the labels for the related q_no.\n",
    "    train_labels = labels.loc[labels.q==q_no].set_index('session').loc[train_users]\n",
    "    valid_labels = labels.loc[labels.q==q_no].set_index('session').loc[valid_users]\n",
    "\n",
    "    # Add the label to the filtered datasets.\n",
    "    train_df[\"correct\"] = train_labels[\"correct\"]\n",
    "    valid_df[\"correct\"] = valid_labels[\"correct\"]\n",
    "\n",
    "    # There's one more step required before we can train the model. \n",
    "    # We need to convert the datatset from Pandas format (pd.DataFrame)\n",
    "    # into TensorFlow Datasets format (tf.data.Dataset).\n",
    "    # TensorFlow Datasets is a high performance data loading library \n",
    "    # which is helpful when training neural networks with accelerators like GPUs and TPUs.\n",
    "    # We are omitting `level_group`, since it is not needed for training anymore.\n",
    "    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"correct\")\n",
    "    valid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_df, label=\"correct\")\n",
    "\n",
    "    # We will now create the Gradient Boosted Trees Model with default settings. \n",
    "    # By default the model is set to train for a classification task.\n",
    "    # gbtm = tfdf.keras.GradientBoostedTreesModel(verbose=0)\n",
    "    # gbtm.compile(metrics=[\"accuracy\"])\n",
    "\n",
    "    # # Train the model.\n",
    "    # gbtm.fit(x=train_ds)\n",
    "\n",
    "    gbmt = load_model(f'Tensorflow_models/model_{q_no}')\n",
    "\n",
    "    # Store the model\n",
    "    models[f'{q_no}'] = gbtm\n",
    "\n",
    "    # Evaluate the trained model on the validation dataset and store the \n",
    "    # evaluation accuracy in the `evaluation_dict`.\n",
    "    inspector = gbtm.make_inspector()\n",
    "    inspector.evaluation()\n",
    "    evaluation = gbtm.evaluate(x=valid_ds,return_dict=True)\n",
    "    evaluation_dict[q_no] = evaluation[\"accuracy\"]         \n",
    "\n",
    "    # Use the trained model to make predictions on the validation dataset and \n",
    "    # store the predicted values in the `prediction_df` dataframe.\n",
    "    predict = gbtm.predict(x=valid_ds)\n",
    "    prediction_df.loc[valid_users, q_no-1] = predict.flatten()\n",
    "    \n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(valid_labels[\"correct\"], (predict > 0.5).astype(int))\n",
    "    f1_scores[q_no] = f1\n",
    "    \n",
    "    # Calculate the AUC score\n",
    "    auc = roc_auc_score(valid_labels[\"correct\"], predict)\n",
    "    auc_scores[q_no] = auc\n",
    "    \n",
    "    \n",
    "    del train_df, valid_df, train_labels, valid_labels\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
